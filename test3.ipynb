{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "FEATURES_ROOT = Path(\"/home/someshs/vid-behaviour/data/study/\")\n",
    "SCENES_URL = \"https://hemingwaydata.blob.core.windows.net/scenes/\"\n",
    "\n",
    "USER_TABLE_STUDY1 = pd.read_csv(FEATURES_ROOT / \"user_data_study1.csv\")\n",
    "USER_TABLE_STUDY2 = pd.read_csv(FEATURES_ROOT / \"user_data_study2.csv\")\n",
    "\n",
    "VIDEO_FEATS_TABLE = pd.read_csv(FEATURES_ROOT / \"video_feats.csv\")\n",
    "COLOR_FEATS = json.load(open(FEATURES_ROOT / \"color_feats.json\"))\n",
    "IMAGE_FEATS = json.load(open(FEATURES_ROOT / \"image_feats.json\"))\n",
    "BLIP_CAPTION = pd.read_csv(FEATURES_ROOT / \"blip_output.csv\")\n",
    "OCR = pd.read_csv(FEATURES_ROOT / \"ocr.csv\")\n",
    "NUM_SCENES= pd.read_csv(FEATURES_ROOT / \"num_scenes.csv\")\n",
    "\n",
    "import ast\n",
    "class AdMemUserVerbalisation:\n",
    "    \n",
    "    def __init__(self, user_id, study_id) -> None:\n",
    "        self.user_id=user_id\n",
    "        self.study_id=study_id\n",
    "        \n",
    "        if study_id==1:\n",
    "            self.df= USER_TABLE_STUDY1\n",
    "        else:\n",
    "            self.df= USER_TABLE_STUDY2\n",
    "        \n",
    "        self.row=self.df.loc[self.df['user_id'] == self.user_id]\n",
    "        \n",
    "    def verb_persona(self) -> str:\n",
    "        '''\n",
    "        pass locale, age, and gender.\n",
    "        HARD CODED FOR NOW\n",
    "        {\n",
    "            'locale': 'India',\n",
    "            'age': '20',\n",
    "        }\n",
    "        gender: given in the user table\n",
    "        '''\n",
    "        age= '20'\n",
    "        gender= self.row['Gender'].values[0]\n",
    "        locale='India'\n",
    "        return f\"{age} year old {gender} from {locale}\"\n",
    "    \n",
    "    def verb_seen_advertisements(self, watched: bool, n: int) -> str:\n",
    "        '''\n",
    "        watched = True/False means the viewer selected/unselected the brand, n is the max number of brands to return.\n",
    "        '''\n",
    "        if watched:\n",
    "            out= self.row['brands_seen'].values[0]\n",
    "            out = ast.literal_eval(out)\n",
    "            out= \", \".join([entry for entry in out])\n",
    "            \n",
    "        else:\n",
    "            seen=ast.literal_eval(self.row['brands_seen'].values[0])\n",
    "            seen_options=   ast.literal_eval(self.row['brands_seen_options'].values[0])\n",
    "            out= set(seen_options)-set(seen)\n",
    "            if len(out) > n:\n",
    "                out= random.sample(out, n)\n",
    "            out= \", \".join([entry for entry in out])\n",
    "        return f\"{out}\"\n",
    "    \n",
    "    def verb_used_products(self, used: bool, n: int) -> str:\n",
    "        '''\n",
    "        watched = True/False means the viewer selected/unselected the brand, n is the max number of brands to return.\n",
    "        '''\n",
    "        if used:\n",
    "            out= self.row['brands_used'].values[0]\n",
    "            out = ast.literal_eval(out)\n",
    "            out= \", \".join([entry for entry in out])\n",
    "            \n",
    "        else:\n",
    "            used=ast.literal_eval(self.row['brands_used'].values[0])\n",
    "            used_options=   ast.literal_eval(self.row['brands_used_options'].values[0])\n",
    "            out= set(used_options)-set(used)\n",
    "            if len(out) > n:\n",
    "                out= random.sample(out, n)\n",
    "           \n",
    "            out= \", \".join([entry for entry in out])\n",
    "        return f\"{out}\"\n",
    "    \n",
    "    def verb_ad_blocker_yt_sub(self) -> str:\n",
    "        '''\n",
    "        The viewer uses an ad blocker and YouTube Premium subscription OR\n",
    "        The viewer uses an ad blocker but is'nt a YouTube Premium subscriber OR\n",
    "        The viewer doesn't use an ad blocker but is a YouTube Premium subscriber OR\n",
    "        The viewer doesn't use an ad blocker and isn't a YouTube Premium subscriber\n",
    "        '''\n",
    "        adblocker= 'uses an' if self.row['ad_block'].values[0] else 'doesn\\'t use an'\n",
    "        youtube_sub= 'is' if self.row['youtube_sub'].values[0] else 'isn\\'t'\n",
    "        return f\"The viewer {adblocker} ad blocker and {youtube_sub} a YouTube Premium subscriber\"\n",
    "    \n",
    "    def verb_time_on_yt_mobile_web(self) -> str:\n",
    "        '''\n",
    "        The viewer spends most of their time on YouTube on mobile OR\n",
    "        The viewer spends most of their time on YouTube on web OR\n",
    "        The viewer spends more time on web than mobile on YouTube OR\n",
    "        The viewer spends more time on mobile than web on YouTube\n",
    "        '''\n",
    "        youtube_mobile= self.row['youtube_mobile'].values[0] \n",
    "        return f\"The time spent by the viewer on Youtube is  {youtube_mobile}\"\n",
    "    \n",
    "    def verb_primary_info_source(self, apprise:bool, n:int) -> str:\n",
    "        '''\n",
    "        apprise = True/False means the viewer selected/unselected the method, n is the max number of methods to return.\n",
    "        '''\n",
    "        apprise= self.row['apprise'].values[0]\n",
    "        \n",
    "        apprise= apprise.split(',')\n",
    "        apprise= random.sample(apprise, n)\n",
    "        apprise= \", \".join([entry for entry in apprise])\n",
    "        return f\"{apprise}\"\n",
    "        \n",
    "    def verbalise(self) -> List[Tuple[str, float]]:\n",
    "        return [\n",
    "            (f\"The viewer is a {self.verb_persona()}.\", 1),\n",
    "            (f\"They have seen advertisements for {self.verb_seen_advertisements(watched=True,n=5)} but not from {self.verb_seen_advertisements(watched=False, n=5)}.\", 1),\n",
    "            (f\"They remember using products like {self.verb_used_products(used=True, n=5)} but not from {self.verb_used_products(used=False, n=5)}.\", 1),\n",
    "            (f\"{self.verb_ad_blocker_yt_sub()}.\", 1),\n",
    "            (f\"{self.verb_time_on_yt_mobile_web()}.\", 1),\n",
    "            (f\"They apprise themselves of the latest products and brands through {self.verb_primary_info_source(True,5)}\", 1),\n",
    "        ]\n",
    "    \n",
    "    def __call__(self) -> str:\n",
    "        entries = self.verbalise()\n",
    "        return \" \".join([entry[0] for entry in entries if random.random() < entry[1]])\n",
    "\n",
    "\n",
    "class AdMemVideoVerbalisation:\n",
    "    def __init__(self, video_id) -> None:\n",
    "        self.video_id = video_id\n",
    "        self.df = VIDEO_FEATS_TABLE\n",
    "        self.row = self.df.loc[self.df[\"video_id\"] == self.video_id]\n",
    "        self.video_length = self.row[\"length\"].values[0]\n",
    "        self.brand = self.row[\"brand\"].values[0]\n",
    "        self.title= self.row[\"title\"].values[0]\n",
    "        self.description= self.row[\"desc\"].values[0]\n",
    "        self.image_feats = IMAGE_FEATS\n",
    "        self.velocity= self.row[\"velocity\"].values[0]\n",
    "        self.scenes_df= NUM_SCENES\n",
    "        self.num_scenes= self.scenes_df.loc[self.scenes_df[\"Video id\"] == self.video_id][\"num_scenes\"].values[0]\n",
    "        \n",
    "    def get_orientation(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns the orientation of the video.\n",
    "        \"\"\"\n",
    "        self.image_name = str(self.video_id) + \"-\" + '001' + \".jpg\"\n",
    "        self.url = SCENES_URL + self.image_name\n",
    "        json_data = self.image_feats[self.url][\"Visual Tags\"]\n",
    "        for item in json_data:\n",
    "            if item[\"category\"] == \"orientation\":\n",
    "                orient_tag = item[\"tag\"]\n",
    "                break\n",
    "        return orient_tag\n",
    "       \n",
    "    def num_people(self) -> Tuple[int, str]:\n",
    "        \"\"\"\n",
    "        Returns the number of unique faces in the video and the celebrities in the video if any else blank.\n",
    "        \"\"\"\n",
    "        return \"\",\"\"\n",
    "    def verbalise(self) -> List[Tuple[str, float]]:\n",
    "        video_pre = [\n",
    "            (\n",
    "                f\"They watch a {self.video_length} second advertisement for {self.brand}.\",\n",
    "                1,\n",
    "            ),\n",
    "            (f\"The title of the advertisement is {self.title}.\", 1),\n",
    "            (f\"The description of the advertisement is {self.description}.\", 1),\n",
    "            (\n",
    "                f\"The ad is shot in {self.get_orientation()} orientation, at a {self.velocity} pace\",\n",
    "                1,\n",
    "            ),\n",
    "            (f\"Following are the description of each scene\", 1),\n",
    "            (f\"There are {self.num_people()[0]} unique faces in the advertisement {self.num_people()[1]}.\", 1),\n",
    "        ]\n",
    "        scenes = [\n",
    "            AdSceneVerbalisation(str(self.video_id), str(scene_id))()\n",
    "            for scene_id in range(1, self.num_scenes + 1)\n",
    "        ]\n",
    "        for scene in scenes:\n",
    "            video_pre.extend((scene, 1))\n",
    "        return video_pre\n",
    "\n",
    "    def __call__(self) -> List[str]:\n",
    "        return \" \".join(\n",
    "            [entry[0] for entry in self.verbalise() if random.random() < entry[1]]\n",
    "        )\n",
    "\n",
    "\n",
    "class AdSceneVerbalisation:\n",
    "    \"\"\"\n",
    "    color_feats format\n",
    "    {'Color Tags': {'background': {'colors': {'Off_White': {'coverage': 0.4926, 'rgb': {'blue': 222, 'green': 219, 'red': 221}}, 'Silver': {'coverage': 0.1803, 'rgb': {'blue': 211, 'green': 207, 'red': 210}}, 'White': {'coverage': 0.327, 'rgb': {'blue': 243, 'green': 241, 'red': 243}}}, 'tones': {'cool': 0, 'neutral': 1.0, 'warm': 0}}, 'foreground': {'colors': {'Black': {'coverage': 0.2535, 'rgb': {'blue': 40, 'green': 35, 'red': 38}}, 'Dark_Gray': {'coverage': 0.1868, 'rgb': {'blue': 70, 'green': 63, 'red': 66}}, 'Gray': {'coverage': 0.2196, 'rgb': {'blue': 156, 'green': 144, 'red': 150}}, 'Off_White': {'coverage': 0.0633, 'rgb': {'blue': 228, 'green': 225, 'red': 227}}, 'White': {'coverage': 0.2768, 'rgb': {'blue': 253, 'green': 252, 'red': 253}}}, 'tones': {'cool': 0, 'neutral': 1.0, 'warm': 0}}, 'overall': {'colors': {'Black': {'coverage': 0.0731, 'rgb': {'blue': 39, 'green': 35, 'red': 37}}, 'Gray': {'coverage': 0.0699, 'rgb': {'blue': 157, 'green': 145, 'red': 151}}, 'Off_White': {'coverage': 0.3903, 'rgb': {'blue': 222, 'green': 219, 'red': 221}}, 'Silver': {'coverage': 0.1414, 'rgb': {'blue': 211, 'green': 207, 'red': 210}}, 'White': {'coverage': 0.3253, 'rgb': {'blue': 245, 'green': 244, 'red': 245}}}, 'tones': {'cool': 0, 'neutral': 1.0, 'warm': 0}}}}\n",
    "    image_feats format\n",
    "    {'Natural Image Aesthetics': {'Overall Aesthetics': 'Low', 'Balancing Elements': 'Low', 'Color Harmony': 'High', 'Interesting Content': 'Low', 'Shallow Depth of Field': 'Low', 'Good Lighting': 'Low', 'Object Emphasis': 'High', 'Repetition': 'Low', 'Rule Of Thirds': 'Low', 'Symmetry': 'Low', 'Vivid Color': 'Low'}, 'Face Features': {'Face Count': 0, 'Face Features': [], 'Facing': 'No Faces', 'Eyes': 'Unknown'}, 'Clutter': 'Low', 'Human Parts': {'Head': 'invisible', 'Torso': 'invisible', 'Upper Arms': 'invisible', 'Lower Arms': 'invisible', 'Upper Legs': 'invisible', 'Lower Legs': 'invisible'}, 'Visual Tags': [{'category': 'orientation', 'confidence': 1.0, 'tag': 'landscape'}, {'category': 'people', 'confidence': 0.997, 'tag': 'woman'}, {'category': 'none', 'confidence': 0.983, 'tag': 'rinse'}, {'category': 'photography style', 'confidence': 0.88, 'tag': 'product photography'}, {'category': 'none', 'confidence': 0.821, 'tag': 'brushing'}, {'category': 'none', 'confidence': 0.789, 'tag': 'hygiene'}, {'category': 'none', 'confidence': 0.781, 'tag': 'shower'}, {'category': 'none', 'confidence': 0.78, 'tag': 'title'}, {'category': 'none', 'confidence': 0.772, 'tag': 'moisture'}, {'category': 'scene', 'confidence': 0.763, 'tag': 'rainwater'}, {'category': 'none', 'confidence': 0.751, 'tag': 'leak'}, {'category': 'none', 'confidence': 0.748, 'tag': 'filler'}, {'category': 'none', 'confidence': 0.74, 'tag': 'fringe'}, {'category': 'none', 'confidence': 0.739, 'tag': 'cleanliness'}, {'category': 'scene', 'confidence': 0.733, 'tag': 'ripple'}]}\n",
    "    ocr_csv format\n",
    "    image path,text\n",
    "    scenes/2473-018.jpg,\"['Could reduce emissions by', 'Compared to conventional fuels']\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, video_id, scene_id) -> None:\n",
    "        self.video_id = video_id\n",
    "        self.scene_id = scene_id.zfill(3)\n",
    "\n",
    "        self.image_feats = IMAGE_FEATS\n",
    "        self.color_feats = COLOR_FEATS\n",
    "        self.blip_caption = BLIP_CAPTION\n",
    "        self.ocr = OCR\n",
    "        self.image_name = self.video_id + \"-\" + self.scene_id + \".jpg\"\n",
    "        self.url = SCENES_URL + self.image_name\n",
    "\n",
    "    def get_blip_caption(self):\n",
    "        caption = self.blip_caption[self.blip_caption[\"image\"] == self.image_name][\n",
    "            \"caption\"\n",
    "        ].values[0]\n",
    "        caption = caption.split(\",\")[0]\n",
    "        caption = caption.strip(\"'['\")\n",
    "        return caption\n",
    "\n",
    "    def get_photography_style(self):\n",
    "        json_data = self.image_feats[self.url][\"Visual Tags\"]\n",
    "        photography_style = \"\"\n",
    "        for item in json_data:\n",
    "            if item[\"category\"] == \"photography style\":\n",
    "                photography_style = item[\"tag\"]\n",
    "                break\n",
    "        \n",
    "        return photography_style\n",
    "\n",
    "    def get_clutter(self):\n",
    "        clutter = self.image_feats[self.url][\"Clutter\"]\n",
    "        return clutter\n",
    "\n",
    "    def get_body_parts(self):\n",
    "        x = self.image_feats[self.url][\"Human Parts\"]\n",
    "        body_parts = []\n",
    "        for i in x.keys():\n",
    "            if x[i] == \"visible\":\n",
    "                body_parts.append(i)\n",
    "        body_parts = \",\".join(body_parts)\n",
    "        return body_parts\n",
    "\n",
    "    def get_num_faces(self):\n",
    "        self.num_faces = self.image_feats[self.url][\"Face Features\"][\"Face Count\"]\n",
    "        if self.num_faces == 0:\n",
    "            return \"There are no prominent faces in the scene.\"\n",
    "        elif self.num_faces == 1:\n",
    "            return \"There is one prominent face in the scene.\"\n",
    "        else:\n",
    "            return f\"There are {self.num_faces} faces in the scene.\"\n",
    "\n",
    "    def get_colors(self, max_colors=5, min_coverage=0.9):\n",
    "        \"\"\"\n",
    "        Keep adding colors till the coverage is >= min_coverage or the number of colors is >= max_colors\n",
    "        \"\"\"\n",
    "        background, foreground = (\n",
    "            self.color_feats[self.url][\"Color Tags\"][\"background\"][\"colors\"],\n",
    "            self.color_feats[self.url][\"Color Tags\"][\"foreground\"][\"colors\"],\n",
    "        )\n",
    "        # sort colors by coverage\n",
    "        background = sorted(\n",
    "            background.items(), key=lambda x: x[1][\"coverage\"], reverse=True\n",
    "        )\n",
    "        foreground = sorted(\n",
    "            foreground.items(), key=lambda x: x[1][\"coverage\"], reverse=True\n",
    "        )\n",
    "        background_colors, foreground_colors = [], []\n",
    "        background_coverage, foreground_coverage = 0, 0\n",
    "        for color in background:\n",
    "            background_colors.append(color[0])\n",
    "            background_coverage += color[1][\"coverage\"]\n",
    "            if (\n",
    "                background_coverage >= min_coverage\n",
    "                or len(background_colors) >= max_colors\n",
    "            ):\n",
    "                break\n",
    "        for color in foreground:\n",
    "            foreground_colors.append(color[0])\n",
    "            foreground_coverage += color[1][\"coverage\"]\n",
    "            if (\n",
    "                foreground_coverage >= min_coverage\n",
    "                or len(foreground_colors) >= max_colors\n",
    "            ):\n",
    "                break\n",
    "        return \",\".join(background_colors), \",\".join(foreground_colors)\n",
    "\n",
    "    def get_tones(self):\n",
    "        \"\"\"\n",
    "        The tones are cool, neutral, warm with the values ranging from 0 to 1.\n",
    "        If there is one tone with a value >= 0.5, then that tone is the dominant tone.\n",
    "        \"\"\"\n",
    "        tones = self.color_feats[self.url][\"Color Tags\"][\"overall\"][\"tones\"]\n",
    "        max_tone = max(tones, key=tones.get)\n",
    "        if tones[max_tone] >= 0.5:\n",
    "            return f\"The dominant tone of the scene is {max_tone}.\"\n",
    "        else:\n",
    "            return \"\"\n",
    "\n",
    "    def get_persons(self):\n",
    "        \"\"\"\n",
    "        Return people category from visual tags but  override by face tags genders if confident\n",
    "        \"\"\"\n",
    "        return \"\"\n",
    "\n",
    "    def get_body_parts(self):\n",
    "        \"\"\"\n",
    "        for all visible body parts, return \"{comma separeted body part} are visible in the scene\"\n",
    "        if all are invisible, return \"There are no visible human body parts in the scene\"\n",
    "        \"\"\"\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "    def get_tags(self, C=0.7):\n",
    "        \"\"\"\n",
    "        Return comma separtated tags over C confidence except photography style\n",
    "        if category is not \"none\", \"{category}: {tag}\" else \"{tag}\"\n",
    "        \"\"\"\n",
    "        tags = self.image_feats[self.url][\"Visual Tags\"]\n",
    "        tags = [\n",
    "            f\"{tag['category']}: {tag['tag']}\"\n",
    "            if tag[\"category\"] != \"none\"\n",
    "            else tag[\"tag\"]\n",
    "            for tag in tags\n",
    "            if tag[\"confidence\"] >= C and not (tag[\"category\"] in [\"photography style\", \"orientation\"])\n",
    "        ]\n",
    "        return \",\".join(tags)\n",
    "\n",
    "    def get_ocr(self):\n",
    "        '''\n",
    "        Return full stop separated text in the scene\n",
    "        '''\n",
    "        texts = self.ocr[self.ocr[\"image path\"] == 'scenes/' + self.image_name][\"text\"].values[0]\n",
    "        #convert to list string to list\n",
    "        texts = texts.strip(\"[]\").split(\",\")\n",
    "        texts = [text.strip().strip(\"'\") for text in texts]\n",
    "        texts = \"'\" + \"', '\".join(texts) + \"'\"\n",
    "        if len(texts) < 3:\n",
    "            return \"There is no text in the scene\"\n",
    "        else:\n",
    "            return \"The text shown in the scene is \" + texts\n",
    "\n",
    "    def get_asr(self):\n",
    "        \"\"\"\n",
    "        \"The narration in the scene is {comma separated ASR} in a {MALE/FEMALE} voice\" if there is narration else return \"There is no narration in the scene\"\n",
    "        \"\"\"\n",
    "        return \"\"\n",
    "\n",
    "    def get_scene_ranking(self):\n",
    "        \"\"\"\n",
    "        Divide into 3 parts based on the scene ranking\n",
    "        First -> \"This scene is {/2nd/3rd..} most viewed\"\n",
    "        Second -> \"\"\n",
    "        Third -> \"This scene is {/2nd/3rd..} least viewed\"\n",
    "        \"\"\"\n",
    "        return \"\"\n",
    "\n",
    "    def verbalise(self) -> List[Tuple[str, float]]:\n",
    "        return [\n",
    "            (f\"The scene shows {self.get_blip_caption()}.\", 1),\n",
    "            (\n",
    "                f\"The foreground colors of the scene are {self.get_colors()[0]} and the background colors are {self.get_colors()[1]}.\",\n",
    "                1,\n",
    "            ),\n",
    "            (self.get_tones(), 1),\n",
    "            (\n",
    "                f\"The photography style of the scene is {self.get_photography_style()}.\",\n",
    "                1,\n",
    "            ),\n",
    "            (f\"The clutter in the scene is {self.get_clutter()}.\", 1),\n",
    "            (f\"The scene has {self.get_persons()} .\", 1),\n",
    "            (f\"{self.get_body_parts()}.\", 1),\n",
    "            (f\"{self.get_num_faces()}\", 1),\n",
    "            (f\"This scene is categorized by the tags  {self.get_tags()}.\", 1),\n",
    "            (f\"{self.get_ocr()}.\", 1),\n",
    "            (f\"{self.get_asr()}\", 1),\n",
    "            (f\"{self.get_scene_ranking()}\", 1),\n",
    "        ]\n",
    "\n",
    "    def __call__(self) -> List[str]:\n",
    "        entries = self.verbalise()\n",
    "        return \" \".join([entry[0] for entry in entries if random.random() < entry[1]])\n",
    "\n",
    "\n",
    "class AdMemResponseVerbalisation:\n",
    "    def __init__(self, user_id, study_id, video_id) -> None:\n",
    "        self.user_id= user_id\n",
    "        self.study_id= study_id\n",
    "        self.video_id= video_id\n",
    "        self.df= pd.read_csv(f'/home/harini/response{study_id}.csv')\n",
    "        \n",
    "        \n",
    "    def brand_recall_verbalise(self) -> str:\n",
    "        recalled= self.df[(self.df['user_id'] == self.user_id) &  (self.df['video_id'] == self.video_id)]['recalled'].values[0]\n",
    "        \n",
    "        desc=self.df[(self.df['user_id'] == self.user_id) &  (self.df['video_id'] == self.video_id)]['scene_description'].values[0]\n",
    "        if recalled == 1:\n",
    "            response=\"The user was able to recall the ad.\"+ f\" The user recalled: \\\"{desc}\\\"\"\n",
    "            \n",
    "        else:\n",
    "            response=\"The user was not able to recall the ad.\"\n",
    "        return response\n",
    "    def scene_recall_verbalise(self) -> str:\n",
    "        pass\n",
    "    def __call__(self) -> List[str]:\n",
    "        return self.verbalise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'float' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5680c0423598>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAdMemVideoVerbalisation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-f1b6418014d5>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         return \" \".join(\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbalise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         )\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f1b6418014d5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         return \" \".join(\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbalise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         )\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'float' and 'str'"
     ]
    }
   ],
   "source": [
    "AdMemVideoVerbalisation(2)()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The scene shows a man and a woman sitting in a car while they drive. The foreground colors of the scene are Black,Dark_Gray and the background colors are Dark_Brown,Gray,Dark_Blue,Black,Brown. The dominant tone of the scene is neutral. The photography style of the scene is editorial photography. The clutter in the scene is High. The scene has  . . There are 2 faces in the scene. This scene is categorized by the tags  people: woman,people: person,people: man,passenger. There is no text in the scene.  '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdSceneVerbalisation('2','2')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
